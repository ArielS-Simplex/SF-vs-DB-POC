{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T14:40:59.223154Z",
     "start_time": "2024-12-30T14:40:59.044107Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a7f1f3-5701-4144-a593-0684309372ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TARGET_TABLE = dbutils.widgets.get(\"TARGET_TABLE\")\n",
    "\n",
    "curr_timestamp = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15bb0dc-f098-41f7-8dff-74bf9a435566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./data_utility_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0198d8f1-5156-4692-afeb-45a99e6bd12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./custom_etl_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d013bc96-6326-42ca-838a-452ce7faa8b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_mgr = SchemaManager(spark)\n",
    "\n",
    "# Get metadata values\n",
    "checkpoint_time = schema_mgr.get_metadata(TARGET_TABLE, \"checkpoint\")\n",
    "source_table = schema_mgr.get_metadata(TARGET_TABLE, \"source_table\")\n",
    "table_keys = schema_mgr.get_metadata(TARGET_TABLE, \"table_keys\").split(\",\")\n",
    "ncp_schema = schema_mgr.get_schema(TARGET_TABLE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb17692-3340-49ff-a0d1-373d359eb77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, from_utc_timestamp, col\n",
    "\n",
    "sync_point_column = \"inserted_at\"\n",
    "print(f\"Last Checkpoint: {checkpoint_time}\")\n",
    "      \n",
    "source_df = (\n",
    "    spark.read.table(source_table)\n",
    "    .where(col(sync_point_column) > checkpoint_time)\n",
    "    .drop(\"inserted_at\").drop(\"source_file_path\").drop(\"source_file_name\")\n",
    "    .withColumn(\"inserted_at\", from_utc_timestamp(current_timestamp(), \"GMT\"))\n",
    "    .dropDuplicates(table_keys)\n",
    ")\n",
    "\n",
    "total_rows = source_df.count()\n",
    "\n",
    "print(f\"Run timestamp: {curr_timestamp} - Total rows expected {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00cb4e8-0037-46ae-b46d-be47313ee46d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if TARGET_TABLE == 'ws_ml_databricks_prod_uks.ncp.transactions_silver':\n",
    "    schema = spark.table(TARGET_TABLE).schema\n",
    "    source_df = filter_and_transform_transactions(df=source_df, schema=ncp_schema)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b7a120-3ae9-4ba2-ba6e-247278d71d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "\n",
    "if spark.catalog.tableExists(TARGET_TABLE):\n",
    "    merge_keys_string = \" AND \".join([f\"target.{t} = source.{t}\" for t in table_keys])\n",
    "    \n",
    "    # Get target table schema fields\n",
    "    target_fields = set(f.name for f in spark.table(TARGET_TABLE).schema.fields)\n",
    "    source_fields = set(f.name for f in source_df.schema.fields)\n",
    "    \n",
    "    # Identify new columns in source\n",
    "    new_columns = source_fields - target_fields\n",
    "\n",
    "    if new_columns:\n",
    "        add_cols_sql = \", \".join(\n",
    "            f\"{col} {source_df.schema[col].dataType.simpleString()}\" for col in new_columns\n",
    "        )\n",
    "        spark.sql(f\"ALTER TABLE {TARGET_TABLE} ADD COLUMNS ({add_cols_sql})\")\n",
    "\n",
    "    # Perform the merge\n",
    "    target = DeltaTable.forName(spark, TARGET_TABLE)\n",
    "    target.alias(\"target\").merge(\n",
    "        source_df.alias(\"source\"), merge_keys_string\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "else:\n",
    "    source_df.write.format(\"delta\").saveAsTable(TARGET_TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48fe7824-3fd3-4c1b-994a-b2281936e468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if total_rows > 0:\n",
    "  schema_mgr.update_metadata(TARGET_TABLE, \"checkpoint\", str(curr_timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98159159-f374-4467-b6f6-7ae0cda7b7cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optimize the target table\n",
    "result = spark.sql(f\"OPTIMIZE {TARGET_TABLE}\")\n",
    "\n",
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4684579901670243,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_batch_etl",
   "widgets": {
    "TARGET_TABLE": {
     "currentValue": "ws_ml_databricks_prod_uks.ncp.transactions_silver",
     "nuid": "e08f5219-c56a-4235-915e-ba596c2126f0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Target Table",
      "name": "TARGET_TABLE",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Target Table",
      "name": "TARGET_TABLE",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
