{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d98908fb-a2b8-44ec-8632-02e874fa81bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Metadata Managment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType, ShortType,\n",
    "    ByteType, BooleanType, FloatType, DoubleType, DecimalType, DateType,\n",
    "    TimestampType, BinaryType, ArrayType, MapType\n",
    ")\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class SchemaManager:\n",
    "    def __init__(self, spark, metadata_table=\"ncp.metadata_table\"):\n",
    "        self.spark = spark\n",
    "        self.metadata_table = f\"{spark.catalog.currentCatalog()}.{metadata_table}\"\n",
    "        self.type_mapping = {\n",
    "            \"bigint\": LongType(),\n",
    "            \"int\": IntegerType(),\n",
    "            \"integer\": IntegerType(),\n",
    "            \"tinyint\": ByteType(),\n",
    "            \"long\": LongType(),\n",
    "            \"smallint\": ShortType(),\n",
    "            \"boolean\": BooleanType(),\n",
    "            \"bit\": BooleanType(),\n",
    "            \"decimal\": lambda p, s: DecimalType(p, s),\n",
    "            \"numeric\": lambda p, s: DecimalType(p, s),\n",
    "            \"float\": FloatType(),\n",
    "            \"real\": FloatType(),\n",
    "            \"double\": DoubleType(),\n",
    "            \"char\": StringType(),\n",
    "            \"string\": StringType(),\n",
    "            \"varchar\": StringType(),\n",
    "            \"nvarchar\": StringType(),\n",
    "            \"text\": StringType(),\n",
    "            \"date\": DateType(),\n",
    "            \"datetime\": TimestampType(),\n",
    "            \"timestamp\": TimestampType(),\n",
    "            \"time\": StringType(),\n",
    "            \"blob\": BinaryType(),\n",
    "            \"binary\": BinaryType()\n",
    "        }\n",
    "        self._create_metadata_table_if_not_exists()\n",
    "\n",
    "    def _create_metadata_table_if_not_exists(self):\n",
    "        \"\"\"Ensure metadata table exists with required fields.\"\"\"\n",
    "        self.spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {self.metadata_table} (\n",
    "                table_name STRING,\n",
    "                schema_json STRING,\n",
    "                checkpoint TIMESTAMP,\n",
    "                source_table STRING,\n",
    "                table_keys STRING\n",
    "            ) USING DELTA\n",
    "        \"\"\")\n",
    "\n",
    "    def update_metadata(self, table_name, field_name, field_value):\n",
    "        \"\"\"Generic function to update a metadata field.\"\"\"\n",
    "        valid_fields = {\"schema_json\", \"checkpoint\", \"source_table\", \"table_keys\"}\n",
    "        if field_name not in valid_fields:\n",
    "            raise ValueError(f\"Invalid metadata field: {field_name}. Must be one of {valid_fields}.\")\n",
    "\n",
    "        # Format value based on type\n",
    "        if isinstance(field_value, str):\n",
    "            value_expr = f\"'{field_value}'\"\n",
    "        elif isinstance(field_value, datetime):  # Convert datetime to timestamp\n",
    "            value_expr = f\"TIMESTAMP('{field_value.isoformat()}')\"\n",
    "        else:\n",
    "            value_expr = str(field_value)\n",
    "\n",
    "        self.spark.sql(f\"\"\"\n",
    "            MERGE INTO {self.metadata_table} AS target\n",
    "            USING (SELECT '{table_name}' AS table_name, {value_expr} AS {field_name}) AS source\n",
    "            ON target.table_name = source.table_name\n",
    "            WHEN MATCHED THEN UPDATE SET target.{field_name} = source.{field_name}\n",
    "            WHEN NOT MATCHED THEN INSERT (table_name, {field_name}) \n",
    "            VALUES (source.table_name, source.{field_name})\n",
    "        \"\"\")\n",
    "\n",
    "    def add_schema(self, table_name, schema_dict):\n",
    "        \"\"\"Add or update a schema definition in the metadata table.\"\"\"\n",
    "        schema_json = json.dumps(schema_dict)\n",
    "        self.update_metadata(table_name, \"schema_json\", schema_json)\n",
    "\n",
    "    def get_schema(self, table_name):\n",
    "        \"\"\"Retrieve the schema as a StructType for a given table.\"\"\"\n",
    "        query = f\"\"\"\n",
    "            SELECT schema_json FROM {self.metadata_table} \n",
    "            WHERE table_name = '{table_name}'\n",
    "        \"\"\"\n",
    "        schema_row = self.spark.sql(query).collect()\n",
    "\n",
    "        if not schema_row:\n",
    "            print(f\"Schema for table '{table_name}' not found.\")\n",
    "            return None\n",
    "\n",
    "        schema_json = schema_row[0][\"schema_json\"]\n",
    "        schema_dict = json.loads(schema_json)\n",
    "        struct_fields = []\n",
    "        for col_name, col_type in schema_dict.items():\n",
    "            if \"decimal\" in col_type:\n",
    "                p, s = map(int, col_type.replace(\"decimal(\", \"\").replace(\")\", \"\").split(\",\"))\n",
    "                struct_fields.append(StructField(col_name, DecimalType(p, s), True))\n",
    "            else:\n",
    "                struct_fields.append(StructField(col_name, self.type_mapping[col_type], True))\n",
    "\n",
    "        return StructType(struct_fields)\n",
    "\n",
    "    def get_metadata(self, table_name, field_name):\n",
    "        \"\"\"Fetch any metadata field except schema_json.\"\"\"\n",
    "        valid_fields = {\"checkpoint\", \"source_table\", \"table_keys\"}\n",
    "        if field_name not in valid_fields:\n",
    "            raise ValueError(f\"Invalid metadata field: {field_name}. Must be one of {valid_fields}.\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT {field_name} FROM {self.metadata_table} \n",
    "            WHERE table_name = '{table_name}'\n",
    "        \"\"\"\n",
    "        result = self.spark.sql(query).collect()\n",
    "\n",
    "        if not result:\n",
    "            print(f\"No value found for '{field_name}' in table '{table_name}'.\")\n",
    "            return None\n",
    "\n",
    "        return result[0][field_name]  # Directly return array, timestamp, or string\n",
    "\n",
    "    def add_new_table_etl(self, schema_name, schema_dict, metadata_updates):\n",
    "\n",
    "        # Add schema\n",
    "        self.add_schema(schema_name, schema_dict)\n",
    "\n",
    "        # Update metadata\n",
    "        for key, value in metadata_updates.items():\n",
    "            self.update_metadata(schema_name, key, value)\n",
    "\n",
    "        # Get metadata values\n",
    "        metadata = {key: schema_mgr.get_metadata(schema_name, key) for key in metadata_updates.keys()}\n",
    "        metadata[\"schema\"] = schema_mgr.get_schema(schema_name)\n",
    "    \n",
    "        return metadata\n",
    "\n",
    "    def list_schemas(self):\n",
    "        \"\"\"List all table names that have schemas stored in the metadata table.\"\"\"\n",
    "        return [row[\"table_name\"] for row in self.spark.sql(f\"\"\"\n",
    "            SELECT table_name FROM {self.metadata_table}\n",
    "        \"\"\").collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0db9bc6-cf56-4606-a74d-d39171bab792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class DatabricksSecretManager:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DatabricksSecretManager with a base URL and retrieves a personal access token.\n",
    "        \"\"\"\n",
    "        base_url= dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "        api_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = api_token\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "    def _post(self, endpoint, payload):\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        response = requests.post(url, headers=self.headers, json=payload)\n",
    "        return response\n",
    "\n",
    "    def create_secret_scope(self, scope_name):\n",
    "        \"\"\"\n",
    "        Creates a new secret scope.\n",
    "        \"\"\"\n",
    "        payload = {\"scope\": scope_name}\n",
    "        response = self._post(\"/api/2.0/secrets/scopes/create\", payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Secret scope '{scope_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to create secret scope '{scope_name}'. Status: {response.status_code}, Response: {response.text}\")\n",
    "\n",
    "    def put_secret(self, scope, key, value):\n",
    "        \"\"\"\n",
    "        Adds or updates a secret in the specified scope.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"scope\": scope,\n",
    "            \"key\": key,\n",
    "            \"string_value\": value\n",
    "        }\n",
    "        response = self._post(\"/api/2.0/secrets/put\", payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Secret '{key}' added to scope '{scope}' successfully.\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to add secret '{key}'. Status: {response.status_code}, Response: {response.text}\")\n",
    "\n",
    "    def update_secret(self, scope, key, new_value):\n",
    "        \"\"\"\n",
    "        Updates an existing secret key by overwriting it with a new value.\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Updating secret '{key}' in scope '{scope}'...\")\n",
    "        self.put_secret(scope, key, new_value)\n",
    "    \n",
    "    def grant_permission(self, scope, principal, permission):\n",
    "        \"\"\"\n",
    "        Grants permission on a secret scope to a user, group, or service principal.\n",
    "\n",
    "        Args:\n",
    "            scope (str): The secret scope name.\n",
    "            principal (str): User or group to grant permissions (e.g. 'user@example.com' or 'users').\n",
    "            permission (str): One of 'READ', 'WRITE', 'MANAGE'.\n",
    "        \"\"\"\n",
    "        valid_permissions = {\"READ\", \"WRITE\", \"MANAGE\"}\n",
    "        if permission not in valid_permissions:\n",
    "            raise ValueError(f\"Invalid permission '{permission}'. Must be one of {valid_permissions}\")\n",
    "\n",
    "        payload = {\n",
    "            \"scope\": scope,\n",
    "            \"principal\": principal,\n",
    "            \"permission\": permission\n",
    "        }\n",
    "        response = self._post(\"/api/2.0/secrets/acls/put\", payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ Granted '{permission}' permission on scope '{scope}' to '{principal}'.\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to grant permission. Status: {response.status_code}, Response: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90ea4675-b0ef-4140-b38d-69b7dc2823a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_cloud_provider():\n",
    "    browser_hostname = spark.conf.get(\"spark.databricks.workspaceUrl\", \"unknown\")\n",
    "\n",
    "    if \"azuredatabricks.net\" in browser_hostname:\n",
    "        cloud = \"Azure\"\n",
    "    elif \"gcp\" in browser_hostname:\n",
    "        cloud = \"GCP\"\n",
    "    elif \"databricks\" in browser_hostname or \"amazonaws.com\" in browser_hostname:\n",
    "        cloud = \"AWS\"\n",
    "    else:\n",
    "        cloud = \"Unknown\"\n",
    "\n",
    "    print(f\"Running on: {cloud}\")\n",
    "    return cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf03526-5451-4c39-b6d6-9e239d6a38aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# per = DatabricksSecretManager()\n",
    "\n",
    "# per.create_secret_scope(\"azure-secret-scope\")\n",
    "# per.put_secret(\"azure-secret-scope\", \"AZURE_OPEN_AI_API_KEY_4_1\", \"\")\n",
    "# per.grant_permission(\"azure-secret-scope\", \"ARND/BA Users\", \"READ\")\n",
    "# per.grant_permission(\"azure-secret-scope\", \"Admins\", \"READ\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_utility_modules",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
